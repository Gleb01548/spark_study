{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://c7ef061a49b3:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3d58342110>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(0, 10000, 1, 8)\n",
    "print(df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "+---+\n",
      "only showing top 0 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "sparkHost = \"sparky\"\n",
    "\n",
    "for x in range(4040, 4060):\n",
    "    link = \"http://\" + sparkHost + \":\" + str(x) + \"/environment/\"\n",
    "    try:\n",
    "        f = urllib.request.urlopen(link)\n",
    "        myfile = f.read()\n",
    "        if sc.applicationId in str(myfile):\n",
    "            print(\"Application ID found on port \", x)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.uiWebUrl.fdel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The DataFrame API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разные способы определить схему"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"author\", StringType(), False),\n",
    "        StructField(\"title\", StringType(), False),\n",
    "        StructField(\"pages\", IntegerType(), False),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"author STRING, title STRING, pages INT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"`Id` INT, `First` STRING, `Last` STRING, `Url` STRING, `Published` STRING, `Hits` INT, `Campaigns` ARRAY<STRING>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    [\n",
    "        1,\n",
    "        \"Jules\",\n",
    "        \"Damji\",\n",
    "        \"https://tinyurl.1\",\n",
    "        \"1/4/2016\",\n",
    "        4535,\n",
    "        [\"twitter\", \"LinkedIn\"],\n",
    "    ],\n",
    "    [\n",
    "        2,\n",
    "        \"Brooke\",\n",
    "        \"Wenig\",\n",
    "        \"https://tinyurl.2\",\n",
    "        \"5/5/2018\",\n",
    "        8908,\n",
    "        [\"twitter\", \"LinkedIn\"],\n",
    "    ],\n",
    "    [\n",
    "        3,\n",
    "        \"Denny\",\n",
    "        \"Lee\",\n",
    "        \"https://tinyurl.3\",\n",
    "        \"6/7/2019\",\n",
    "        7659,\n",
    "        [\"web\", \"twitter\", \"FB\", \"LinkedIn\"],\n",
    "    ],\n",
    "    [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568, [\"twitter\", \"FB\"]],\n",
    "    [\n",
    "        5,\n",
    "        \"Matei\",\n",
    "        \"Zaharia\",\n",
    "        \"https://tinyurl.5\",\n",
    "        \"5/14/2014\",\n",
    "        40578,\n",
    "        [\"web\", \"twitter\", \"FB\", \"LinkedIn\"],\n",
    "    ],\n",
    "    [\n",
    "        6,\n",
    "        \"Reynold\",\n",
    "        \"Xin\",\n",
    "        \"https://tinyurl.6\",\n",
    "        \"3/2/2015\",\n",
    "        25568,\n",
    "        [\"twitter\", \"LinkedIn\"],\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"Id\", IntegerType(), False),\n",
    "        StructField(\"First\", StringType(), False),\n",
    "        StructField(\"Last\", StringType(), False),\n",
    "        StructField(\"Url\", StringType(), False),\n",
    "        StructField(\"Published\", StringType(), False),\n",
    "        StructField(\"Hits\", IntegerType(), False),\n",
    "        StructField(\"Campaigns\", ArrayType(StringType()), False),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogs_df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = false)\n",
      " |-- First: string (nullable = false)\n",
      " |-- Last: string (nullable = false)\n",
      " |-- Url: string (nullable = false)\n",
      " |-- Published: string (nullable = false)\n",
      " |-- Hits: integer (nullable = false)\n",
      " |-- Campaigns: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('Id', IntegerType(), False), StructField('First', StringType(), False), StructField('Last', StringType(), False), StructField('Url', StringType(), False), StructField('Published', StringType(), False), StructField('Hits', IntegerType(), False), StructField('Campaigns', ArrayType(StringType(), True), False)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "функциии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Id', 'First', 'Last', 'Url', 'Published', 'Hits', 'Campaigns']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs_df.select(F.expr(\"Hits * 2\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs_df.select(F.col(\"Hits\") * 2).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|   AuthorsId|\n",
      "+------------+\n",
      "| JulesDamji1|\n",
      "|BrookeWenig2|\n",
      "|   DennyLee3|\n",
      "+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs_df.withColumn(\n",
    "    \"AuthorsId\", (F.concat(F.expr(\"First\"), F.expr(\"Last\"), F.expr(\"Id\")))\n",
    ").select(\"AuthorsId\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем таблицу и определяем схему"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "fire_schema = StructType(\n",
    "    [\n",
    "        StructField(\"CallNumber\", IntegerType(), True),\n",
    "        StructField(\"UnitID\", StringType(), True),\n",
    "        StructField(\"IncidentNumber\", IntegerType(), True),\n",
    "        StructField(\"CallType\", StringType(), True),\n",
    "        StructField(\"CallDate\", StringType(), True),\n",
    "        StructField(\"WatchDate\", StringType(), True),\n",
    "        StructField(\"CallFinalDisposition\", StringType(), True),\n",
    "        StructField(\"AvailableDtTm\", StringType(), True),\n",
    "        StructField(\"Address\", StringType(), True),\n",
    "        StructField(\"City\", StringType(), True),\n",
    "        StructField(\"Zipcode\", IntegerType(), True),\n",
    "        StructField(\"Battalion\", StringType(), True),\n",
    "        StructField(\"StationArea\", StringType(), True),\n",
    "        StructField(\"Box\", StringType(), True),\n",
    "        StructField(\"OriginalPriority\", StringType(), True),\n",
    "        StructField(\"Priority\", StringType(), True),\n",
    "        StructField(\"FinalPriority\", IntegerType(), True),\n",
    "        StructField(\"ALSUnit\", BooleanType(), True),\n",
    "        StructField(\"CallTypeGroup\", StringType(), True),\n",
    "        StructField(\"NumAlarms\", IntegerType(), True),\n",
    "        StructField(\"UnitType\", StringType(), True),\n",
    "        StructField(\"UnitSequenceInCallDispatch\", IntegerType(), True),\n",
    "        StructField(\"FirePreventionDistrict\", StringType(), True),\n",
    "        StructField(\"SupervisorDistrict\", StringType(), True),\n",
    "        StructField(\"Neighborhood\", StringType(), True),\n",
    "        StructField(\"Location\", StringType(), True),\n",
    "        StructField(\"RowID\", StringType(), True),\n",
    "        StructField(\"Delay\", FloatType(), True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_fire_file = \"./LearningSparkV2/chapter3/data/sf-fire-calls.csv\"\n",
    "fire_df = spark.read.csv(sf_fire_file, header=True, schema=fire_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------+\n",
      "|CallNumber|UnitID|IncidentNumber|        CallType|  CallDate| WatchDate|CallFinalDisposition|       AvailableDtTm|             Address|City|Zipcode|Battalion|StationArea| Box|OriginalPriority|Priority|FinalPriority|ALSUnit|CallTypeGroup|NumAlarms|      UnitType|UnitSequenceInCallDispatch|FirePreventionDistrict|SupervisorDistrict|        Neighborhood|            Location|        RowID|    Delay|\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------+\n",
      "|  20110016|   T13|       2003235|  Structure Fire|01/11/2002|01/10/2002|               Other|01/11/2002 01:51:...|2000 Block of CAL...|  SF|  94109|      B04|         38|3362|               3|       3|            3|  false|         NULL|        1|         TRUCK|                         2|                     4|                 5|     Pacific Heights|(37.7895840679362...|020110016-T13|     2.95|\n",
      "|  20110022|   M17|       2003241|Medical Incident|01/11/2002|01/10/2002|               Other|01/11/2002 03:01:...|0 Block of SILVER...|  SF|  94124|      B10|         42|6495|               3|       3|            3|   true|         NULL|        1|         MEDIC|                         1|                    10|                10|Bayview Hunters P...|(37.7337623673897...|020110022-M17|      4.7|\n",
      "|  20110023|   M41|       2003242|Medical Incident|01/11/2002|01/10/2002|               Other|01/11/2002 02:39:...|MARKET ST/MCALLIS...|  SF|  94102|      B03|         01|1455|               3|       3|            3|   true|         NULL|        1|         MEDIC|                         2|                     3|                 6|          Tenderloin|(37.7811772186856...|020110023-M41|2.4333334|\n",
      "|  20110032|   E11|       2003250|    Vehicle Fire|01/11/2002|01/10/2002|               Other|01/11/2002 04:16:...|APPLETON AV/MISSI...|  SF|  94110|      B06|         32|5626|               3|       3|            3|  false|         NULL|        1|        ENGINE|                         1|                     6|                 9|      Bernal Heights|(37.7388432849018...|020110032-E11|      1.5|\n",
      "|  20110043|   B04|       2003259|          Alarms|01/11/2002|01/10/2002|               Other|01/11/2002 06:01:...|1400 Block of SUT...|  SF|  94109|      B04|         03|3223|               3|       3|            3|  false|         NULL|        1|         CHIEF|                         2|                     4|                 2|    Western Addition|(37.7872890372638...|020110043-B04|3.4833333|\n",
      "|  20110072|   T08|       2003279|  Structure Fire|01/11/2002|01/11/2002|               Other|01/11/2002 08:03:...|  BEALE ST/FOLSOM ST|  SF|  94105|      B03|         35|2122|               3|       3|            3|  false|         NULL|        1|         TRUCK|                         2|                     3|                 6|Financial Distric...|(37.7886866619654...|020110072-T08|     1.75|\n",
      "|  20110125|   E33|       2003301|          Alarms|01/11/2002|01/11/2002|               Other|01/11/2002 09:46:...|0 Block of FARALL...|  SF|  94112|      B09|         33|8324|               3|       3|            3|  false|         NULL|        1|        ENGINE|                         2|                     9|                11|Oceanview/Merced/...|(37.7140353531157...|020110125-E33|2.7166667|\n",
      "|  20110130|   E36|       2003304|          Alarms|01/11/2002|01/11/2002|               Other|01/11/2002 09:58:...|600 Block of POLK ST|  SF|  94102|      B02|         03|3114|               3|       3|            3|  false|         NULL|        1|        ENGINE|                         1|                     2|                 6|          Tenderloin|(37.7826266328595...|020110130-E36|1.7833333|\n",
      "|  20110197|   E05|       2003343|Medical Incident|01/11/2002|01/11/2002|               Other|01/11/2002 12:06:...|1500 Block of WEB...|  SF|  94115|      B04|         05|3513|               3|       3|            3|  false|         NULL|        1|        ENGINE|                         1|                     4|                 5|           Japantown|(37.784958590666,...|020110197-E05|1.5166667|\n",
      "|  20110215|   E06|       2003348|Medical Incident|01/11/2002|01/11/2002|               Other|01/11/2002 01:08:...|DIAMOND ST/MARKET ST|  SF|  94114|      B05|         06|5415|               3|       3|            3|  false|         NULL|        1|        ENGINE|                         1|                     5|                 8| Castro/Upper Market|(37.7618954753708...|020110215-E06|2.7666667|\n",
      "|  20110274|   M07|       2003381|Medical Incident|01/11/2002|01/11/2002|               Other|01/11/2002 03:31:...|2700 Block of MIS...|  SF|  94110|      B06|         11|5525|               1|       1|            2|   true|         NULL|        1|         MEDIC|                         1|                     6|                 9|             Mission|(37.7530339738059...|020110274-M07|2.1833334|\n",
      "|  20110275|   T15|       2003382|  Structure Fire|01/11/2002|01/11/2002|               Other|01/11/2002 02:59:...|BRUNSWICK ST/GUTT...|  SF|  94112|      B09|         43|6218|               3|       3|            3|  false|         NULL|        1|         TRUCK|                         1|                     9|                11|           Excelsior|(37.7105545807996...|020110275-T15|      2.5|\n",
      "|  20110304|   E03|       2003399|Medical Incident|01/11/2002|01/11/2002|               Other|01/11/2002 04:22:...|1000 Block of SUT...|  SF|  94109|      B04|         03|1557|               3|       3|            3|  false|         NULL|        1|        ENGINE|                         1|                     4|                 3|            Nob Hill|(37.7881263034393...|020110304-E03|2.4166667|\n",
      "|  20110308|   E14|       2003403|Medical Incident|01/11/2002|01/11/2002|               Other|01/11/2002 04:18:...|100 Block of 21ST...|  SF|  94121|      B07|         14|7173|               3|       3|            3|  false|         NULL|        1|        ENGINE|                         1|                     7|                 1|      Outer Richmond|(37.7850084431077...|020110308-E14|     4.95|\n",
      "|  20110313|   B10|       2003408|  Structure Fire|01/11/2002|01/11/2002|               Other|01/11/2002 04:09:...|700 Block of CAPP ST|  SF|  94110|      B06|         07|5472|               3|       3|            3|  false|         NULL|        1|         CHIEF|                         6|                     6|                 9|             Mission|(37.7547064357942...|020110313-B10|1.4166666|\n",
      "|  20110313|    D3|       2003408|  Structure Fire|01/11/2002|01/11/2002|               Other|01/11/2002 04:09:...|700 Block of CAPP ST|  SF|  94110|      B06|         07|5472|               3|       3|            3|  false|         NULL|        1|         CHIEF|                         4|                     6|                 9|             Mission|(37.7547064357942...| 020110313-D3|2.5333333|\n",
      "|  20110313|   E32|       2003408|  Structure Fire|01/11/2002|01/11/2002|               Other|01/11/2002 04:09:...|700 Block of CAPP ST|  SF|  94110|      B06|         07|5472|               3|       3|            3|   true|         NULL|        1|        ENGINE|                         8|                     6|                 9|             Mission|(37.7547064357942...|020110313-E32|1.8833333|\n",
      "|  20110315|   RC2|       2003409|Medical Incident|01/11/2002|01/11/2002|               Other|01/11/2002 04:34:...|200 Block of LAGU...|  SF|  94116|      B08|         20|8635|               3|       3|            3|   true|         NULL|        1|RESCUE CAPTAIN|                         2|                     8|                 7|  West of Twin Peaks|(37.7501117393668...|020110315-RC2|     5.35|\n",
      "|  20110330|   E14|       2003417|Medical Incident|01/11/2002|01/11/2002|               Other|01/11/2002 04:51:...|BALBOA ST/PARK PR...|  SF|  94118|      B07|         31|7145|               3|       3|            3|  false|         NULL|        1|        ENGINE|                         1|                     7|                 1|      Inner Richmond|(37.7768682293368...|020110330-E14|      2.0|\n",
      "|  20110330|   M12|       2003417|Medical Incident|01/11/2002|01/11/2002|               Other|01/11/2002 04:51:...|BALBOA ST/PARK PR...|  SF|  94118|      B07|         31|7145|               3|       3|            3|   true|         NULL|        1|         MEDIC|                         2|                     7|                 1|      Inner Richmond|(37.7768682293368...|020110330-M12|1.8166667|\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fire_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path file:/home/jovyan/code/owl.csv already exists. Set mode as \"overwrite\" to overwrite the existing path.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jovyan/code/book_learn_spark.ipynb Cell 30\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f7373642f70726f6a656374732f7365617263685f6170692f7079737061726b2d646576636f6e7461696e6572222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f7373642f70726f6a656374732f7365617263685f6170692f7079737061726b2d646576636f6e7461696e65722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d@ssh-remote%2B192.168.0.124/home/jovyan/code/book_learn_spark.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m fire_df\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mformat(\u001b[39m\"\u001b[39;49m\u001b[39mparquet\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49msave(\u001b[39m\"\u001b[39;49m\u001b[39mowl.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1463\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1461\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jwrite\u001b[39m.\u001b[39msave()\n\u001b[1;32m   1462\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1463\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49msave(path)\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/home/jovyan/code/owl.csv already exists. Set mode as \"overwrite\" to overwrite the existing path."
     ]
    }
   ],
   "source": [
    "fire_df.write.format(\"parquet\").save(\"owl.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_df.write.format(\"parquet\").saveAsTable(\"owl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[CallNumber: int, UnitID: string, IncidentNumber: int, CallType: string, CallDate: string, WatchDate: string, CallFinalDisposition: string, AvailableDtTm: string, Address: string, City: string, Zipcode: int, Battalion: string, StationArea: string, Box: string, OriginalPriority: string, Priority: string, FinalPriority: int, ALSUnit: boolean, CallTypeGroup: string, NumAlarms: int, UnitType: string, UnitSequenceInCallDispatch: int, FirePreventionDistrict: string, SupervisorDistrict: string, Neighborhood: string, Location: string, RowID: string, Delay: float]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(\"owl.csv\").csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "df = pq.read_table(\"owl.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+--------------+\n",
      "|IncidentNumber|AvailableDtTm         |CallType      |\n",
      "+--------------+----------------------+--------------+\n",
      "|2003235       |01/11/2002 01:51:44 AM|Structure Fire|\n",
      "|2003250       |01/11/2002 04:16:46 AM|Vehicle Fire  |\n",
      "|2003259       |01/11/2002 06:01:58 AM|Alarms        |\n",
      "|2003279       |01/11/2002 08:03:26 AM|Structure Fire|\n",
      "|2003301       |01/11/2002 09:46:44 AM|Alarms        |\n",
      "+--------------+----------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fire_df.select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\").where(\n",
    "    F.col(\"CallType\") != \"Medical Incident\"\n",
    ").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------+\n",
      "|IncidentNumber|       AvailableDtTm|      CallType|\n",
      "+--------------+--------------------+--------------+\n",
      "|       2003235|01/11/2002 01:51:...|Structure Fire|\n",
      "|       2003250|01/11/2002 04:16:...|  Vehicle Fire|\n",
      "|       2003259|01/11/2002 06:01:...|        Alarms|\n",
      "|       2003279|01/11/2002 08:03:...|Structure Fire|\n",
      "|       2003301|01/11/2002 09:46:...|        Alarms|\n",
      "+--------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fire_df.select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\").where(\n",
    "    F.col(\"CallType\") != \"Medical Incident\"\n",
    ").show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DistinctCallTypes|\n",
      "+-----------------+\n",
      "|               30|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fire_df.select(\"CallType\").where(F.col(\"CallType\").isNotNull()).agg(\n",
    "    F.countDistinct(\"CallType\").alias(\"DistinctCallTypes\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|CallType                     |\n",
      "+-----------------------------+\n",
      "|Elevator / Escalator Rescue  |\n",
      "|Aircraft Emergency           |\n",
      "|Alarms                       |\n",
      "|Odor (Strange / Unknown)     |\n",
      "|Citizen Assist / Service Call|\n",
      "|HazMat                       |\n",
      "|Explosion                    |\n",
      "|Oil Spill                    |\n",
      "|Vehicle Fire                 |\n",
      "|Suspicious Package           |\n",
      "+-----------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fire_df.select(\"CallType\").where(F.col(\"CallType\").isNotNull()).distinct().alias(\n",
    "    \"DistinctCallTypes\"\n",
    ").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_ts_df = (\n",
    "    fire_df.withColumn(\"IncidentDate\", F.to_timestamp(F.col(\"CallDate\"), \"MM/dd/yyyy\"))\n",
    "    .drop(\"CallDate\")\n",
    "    .withColumn(\"OnWatchDate\", F.to_timestamp(F.col(\"WatchDate\"), \"MM/dd/yyyy\"))\n",
    "    .drop(\"WatchDate\")\n",
    "    .withColumn(\n",
    "        \"AvailableDtTS\", F.to_timestamp(F.col(\"AvailableDtTm\"), \"MM/dd/yyyy hh:mm:ss a\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n",
      "|       IncidentDate|        OnWatchDate|      AvailableDtTS|\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 01:51:44|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 03:01:18|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 02:39:50|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 04:16:46|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fire_ts_df.select(\"IncidentDate\", \"OnWatchDate\", \"AvailableDtTS\").show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|year(IncidentDate)|\n",
      "+------------------+\n",
      "|              2000|\n",
      "|              2001|\n",
      "|              2002|\n",
      "|              2003|\n",
      "|              2004|\n",
      "|              2005|\n",
      "|              2006|\n",
      "|              2007|\n",
      "|              2008|\n",
      "|              2009|\n",
      "|              2010|\n",
      "|              2011|\n",
      "|              2012|\n",
      "|              2013|\n",
      "|              2014|\n",
      "|              2015|\n",
      "|              2016|\n",
      "|              2017|\n",
      "|              2018|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fire_ts_df.select(F.year(\"IncidentDate\")).distinct().orderBy(\n",
    "    F.year(\"IncidentDate\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+------+\n",
      "|CallType                       |count |\n",
      "+-------------------------------+------+\n",
      "|Medical Incident               |113794|\n",
      "|Structure Fire                 |23319 |\n",
      "|Alarms                         |19406 |\n",
      "|Traffic Collision              |7013  |\n",
      "|Citizen Assist / Service Call  |2524  |\n",
      "|Other                          |2166  |\n",
      "|Outside Fire                   |2094  |\n",
      "|Vehicle Fire                   |854   |\n",
      "|Gas Leak (Natural and LP Gases)|764   |\n",
      "|Water Rescue                   |755   |\n",
      "+-------------------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fire_ts_df.select(\"CallType\").where(F.col(\"CallType\").isNotNull()).groupBy(\n",
    "    \"CallType\"\n",
    ").count().orderBy(\"count\", ascending=False).show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|            CallType|count(CallType)|\n",
      "+--------------------+---------------+\n",
      "|Elevator / Escala...|            453|\n",
      "|  Aircraft Emergency|             36|\n",
      "|              Alarms|          19406|\n",
      "|Odor (Strange / U...|            490|\n",
      "|Citizen Assist / ...|           2524|\n",
      "|              HazMat|            124|\n",
      "|           Explosion|             89|\n",
      "|           Oil Spill|             21|\n",
      "|        Vehicle Fire|            854|\n",
      "|  Suspicious Package|             15|\n",
      "|Extrication / Ent...|             28|\n",
      "|               Other|           2166|\n",
      "|        Outside Fire|           2094|\n",
      "|   Traffic Collision|           7013|\n",
      "|       Assist Police|             35|\n",
      "|Gas Leak (Natural...|            764|\n",
      "|        Water Rescue|            755|\n",
      "|   Electrical Hazard|            482|\n",
      "|   High Angle Rescue|             32|\n",
      "|      Structure Fire|          23319|\n",
      "+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fire_ts_df.select(\"CallType\").where(F.col(\"CallType\").isNotNull()).groupBy(\n",
    "    \"CallType\"\n",
    ").agg(F.count(\"CallType\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|count(Address)|\n",
      "+--------------+\n",
      "|        175296|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fire_ts_df.select(F.count(\"Address\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+------------------+--------------------+--------------------+--------------------+--------------------+-----------+------------------+---------+------------------+------------------+------------------+------------------+-------------------+--------------------+-------------------+--------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+-----------------+\n",
      "|summary|          CallNumber|            UnitID|    IncidentNumber|            CallType|CallFinalDisposition|       AvailableDtTm|             Address|       City|           Zipcode|Battalion|       StationArea|               Box|  OriginalPriority|          Priority|      FinalPriority|       CallTypeGroup|          NumAlarms|UnitType|UnitSequenceInCallDispatch|FirePreventionDistrict|SupervisorDistrict|        Neighborhood|            Location|        RowID|            Delay|\n",
      "+-------+--------------------+------------------+------------------+--------------------+--------------------+--------------------+--------------------+-----------+------------------+---------+------------------+------------------+------------------+------------------+-------------------+--------------------+-------------------+--------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+-----------------+\n",
      "|  count|              175296|            175296|            175296|              175296|              175296|              173502|              175296|     175089|            175154|   175296|            175221|            175283|            174374|            175296|             175296|               75779|             175296|  175296|                    175295|                175296|            175296|              175296|              175296|       175296|           175296|\n",
      "|   mean|1.0023517638629518E8| 75.09678410773658| 9900785.635057274|                NULL|                NULL|                NULL|                NULL|       NULL| 94113.58601573472|     NULL| 18.08921735922151|   4090.2165596011|2.7605317888321927|2.7449383763374695|  2.792174379335524|                NULL| 1.0049858525009128|    NULL|        2.1280413018055278|    4.7602276974384035| 5.989141976365816|                NULL|                NULL|         NULL|3.892364154521585|\n",
      "| stddev|  5.39690928341264E7|12.818274120753578|5407019.5249005295|                NULL|                NULL|                NULL|                NULL|       NULL|10.221869589797066|     NULL|14.463948601522217|2374.8822257149536|0.5070550169980568|0.5170575577556951|0.40575247533832254|                NULL|0.09811952680742102|    NULL|        2.0543838673391743|    2.9348933918215145|2.7217404283152113|                NULL|                NULL|         NULL|9.378286226254207|\n",
      "|    min|             1030128|                27|             30636|      Administrative|Against Medical A...|01/01/2001 01:28:...|0 Block of 0NB GG...|         BN|             94102|      B01|                01|              0123|                 1|                 1|                  2|               Alarm|                  1| AIRPORT|                         1|                     1|                 1|Bayview Hunters P...|(37.6168823239251...|001030128-B04|      0.016666668|\n",
      "|    max|           183104004|               UU1|          18130302|Watercraft in Dis...|    Unable to Locate|12/31/2017 12:55:...|YOSEMITE AV/MENDE...|Yerba Buena|             94158|      B99|                A3|              AI02|                 I|                 I|                  3|Potentially Life-...|                  5|   TRUCK|                        68|                  None|              None|    Western Addition|(37.8544643401172...|183104004-B01|          1844.55|\n",
      "+-------+--------------------+------------------+------------------+--------------------+--------------------+--------------------+--------------------+-----------+------------------+---------+------------------+------------------+------------------+------------------+-------------------+--------------------+-------------------+--------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fire_ts_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnm_file = \"data/mnm_dataset.csv\"\n",
    "mnm_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(mnm_file)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnm_df.createOrReplaceTempView(\"mnm_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnm_df_new = spark.sql(\n",
    "    \"\"\"SELECT State, Color, Count, sum(Count) AS Total \n",
    "             FROM mnm_df \n",
    "             GROUP BY State, Color, Count\n",
    "             ORDER BY Total DESC             \n",
    "             \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['Total DESC NULLS LAST], true\n",
      "+- 'Aggregate ['State, 'Color, 'Count], ['State, 'Color, 'Count, 'sum('Count) AS Total#328]\n",
      "   +- 'UnresolvedRelation [mnm_df], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "State: string, Color: string, Count: int, Total: bigint\n",
      "Sort [Total#328L DESC NULLS LAST], true\n",
      "+- Aggregate [State#322, Color#323, Count#324], [State#322, Color#323, Count#324, sum(Count#324) AS Total#328L]\n",
      "   +- SubqueryAlias mnm_df\n",
      "      +- View (`mnm_df`, [State#322,Color#323,Count#324])\n",
      "         +- Relation [State#322,Color#323,Count#324] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [Total#328L DESC NULLS LAST], true\n",
      "+- Aggregate [State#322, Color#323, Count#324], [State#322, Color#323, Count#324, sum(Count#324) AS Total#328L]\n",
      "   +- Relation [State#322,Color#323,Count#324] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [Total#328L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(Total#328L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=127]\n",
      "      +- HashAggregate(keys=[State#322, Color#323, Count#324], functions=[sum(Count#324)], output=[State#322, Color#323, Count#324, Total#328L])\n",
      "         +- Exchange hashpartitioning(State#322, Color#323, Count#324, 200), ENSURE_REQUIREMENTS, [plan_id=124]\n",
      "            +- HashAggregate(keys=[State#322, Color#323, Count#324], functions=[partial_sum(Count#324)], output=[State#322, Color#323, Count#324, sum#335L])\n",
      "               +- FileScan csv [State#322,Color#323,Count#324] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/code/data/mnm_dataset.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<State:string,Color:string,Count:int>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnm_df_new.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"SparkSQLExampleApp\").getOrCreate()\n",
    "# spark.conf.set(\"spark.sql.catalogImplementation\", \"hive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"data/departuredelays.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(csv_file)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1011245|    6|     602|   ABE|        ATL|\n",
      "|1020600|   -8|     369|   ABE|        DTW|\n",
      "|1021245|   -2|     602|   ABE|        ATL|\n",
      "|1020605|   -4|     602|   ABE|        ATL|\n",
      "|1031245|   -4|     602|   ABE|        ATL|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceGlobalTempView(\"tab1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"date STRING, delay INT, distance INT, origin STRING, destination STRING\"\n",
    "df = spark.read.csv(csv_file, header=True, schema=schema)\n",
    "df.createOrReplaceGlobalTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\" SELECT distance, origin, destination\n",
    "          FROM global_temp.us_delay_flights_tbl WHERE distance > 1000 \n",
    "          ORDER BY distance DESC\n",
    "          \"\"\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|02190925| 1638|   SFO|        ORD|\n",
      "|01031755|  396|   SFO|        ORD|\n",
      "|01022330|  326|   SFO|        ORD|\n",
      "|01051205|  320|   SFO|        ORD|\n",
      "|01190925|  297|   SFO|        ORD|\n",
      "|02171115|  296|   SFO|        ORD|\n",
      "|01071040|  279|   SFO|        ORD|\n",
      "|01051550|  274|   SFO|        ORD|\n",
      "|03120730|  266|   SFO|        ORD|\n",
      "|01261104|  258|   SFO|        ORD|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "          SELECT date, delay, origin, destination\n",
    "          FROM global_temp.us_delay_flights_tbl\n",
    "          WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD'\n",
    "          ORDER BY delay DESC\n",
    "          \"\"\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+--------------+\n",
      "|delay|origin|destination|Flights_delays|\n",
      "+-----+------+-----------+--------------+\n",
      "|  333|   ABE|        ATL|   Lang Delays|\n",
      "|  305|   ABE|        ATL|   Lang Delays|\n",
      "|  275|   ABE|        ATL|   Lang Delays|\n",
      "|  257|   ABE|        ATL|   Lang Delays|\n",
      "|  247|   ABE|        ATL|   Lang Delays|\n",
      "|  247|   ABE|        DTW|   Lang Delays|\n",
      "|  219|   ABE|        ORD|   Lang Delays|\n",
      "|  211|   ABE|        ATL|   Lang Delays|\n",
      "|  197|   ABE|        DTW|   Lang Delays|\n",
      "|  192|   ABE|        ORD|   Lang Delays|\n",
      "+-----+------+-----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "owl = spark.sql(\n",
    "    \"\"\"\n",
    "          SELECT delay, origin, destination,\n",
    "            CASE \n",
    "                WHEN delay > 360 THEN \"Very Long Delays\"\n",
    "                WHEN delay > 120 AND delay < 360 THEN \"Lang Delays\"\n",
    "                WHEN delay > 60 AND delay < 120 THEN \"Short Delays\"\n",
    "                WHEN delay > 0 and delay < 60 THEN \"Tolerable Delays\"\n",
    "                ELSE \"Early\"\n",
    "            END AS Flights_delays\n",
    "            FROM global_temp.us_delay_flights_tbl\n",
    "            ORDER BY origin, delay DESC              \n",
    "          \"\"\"\n",
    ")\n",
    "owl.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          CREATE OR REPLACE GLOBAL TEMP VIEW us_origin_airport_SFO_global_tmp_view AS\n",
    "          SELECT delay, origin, destination,\n",
    "            CASE \n",
    "                WHEN delay > 360 THEN \"Very Long Delays\"\n",
    "                WHEN delay > 120 AND delay < 360 THEN \"Lang Delays\"\n",
    "                WHEN delay > 60 AND delay < 120 THEN \"Short Delays\"\n",
    "                WHEN delay > 0 and delay < 60 THEN \"Tolerable Delays\"\n",
    "                ELSE \"Early\"\n",
    "            END AS Flights_delays\n",
    "            FROM global_temp.us_delay_flights_tbl\n",
    "            ORDER BY origin, delay DESC              \n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+--------------+\n",
      "|delay|origin|destination|Flights_delays|\n",
      "+-----+------+-----------+--------------+\n",
      "|  333|   ABE|        ATL|   Lang Delays|\n",
      "|  305|   ABE|        ATL|   Lang Delays|\n",
      "|  275|   ABE|        ATL|   Lang Delays|\n",
      "|  257|   ABE|        ATL|   Lang Delays|\n",
      "|  247|   ABE|        ATL|   Lang Delays|\n",
      "|  247|   ABE|        DTW|   Lang Delays|\n",
      "|  219|   ABE|        ORD|   Lang Delays|\n",
      "|  211|   ABE|        ATL|   Lang Delays|\n",
      "|  197|   ABE|        DTW|   Lang Delays|\n",
      "|  192|   ABE|        ORD|   Lang Delays|\n",
      "+-----+------+-----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(    \"\"\"\n",
    "          SELECT * FROM global_temp.us_origin_airport_SFO_global_tmp_view          \n",
    "          \"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"distance\", \"origin\", \"destination\").where(col(\"distance\") > 1000).orderBy(\n",
    "    desc(\"distance\")\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE learn_spark_db\")\n",
    "spark.sql(\"USE learn_spark_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|    learn_spark_db|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_schema()\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='date', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='delay', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='distance', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='origin', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='destination', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()\n",
    "spark.catalog.listTables()\n",
    "spark.catalog.listColumns(\"global_temp.us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CACHE LAZY TABLE global_temp.us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Failed to find data source: avro. Avro is built-in but external data source module since Spark 2.4. Please deploy the application as per the deployment section of Apache Avro Data Source Guide.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jovyan/code/book_learn_spark.ipynb Cell 70\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f7373642f70726f6a656374732f7365617263685f6170692f7079737061726b2d646576636f6e7461696e6572222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f7373642f70726f6a656374732f7365617263685f6170692f7079737061726b2d646576636f6e7461696e65722f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d@ssh-remote%2B192.168.0.124/home/jovyan/code/book_learn_spark.ipynb#Y162sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49mformat(\u001b[39m'\u001b[39;49m\u001b[39mavro\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39m./data/avro/*\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/sql/readwriter.py:307\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[1;32m    306\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 307\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mload(path))\n\u001b[1;32m    308\u001b[0m \u001b[39melif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(path) \u001b[39m!=\u001b[39m \u001b[39mlist\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Failed to find data source: avro. Avro is built-in but external data source module since Spark 2.4. Please deploy the application as per the deployment section of Apache Avro Data Source Guide."
     ]
    }
   ],
   "source": [
    "df = spark.read.format('avro').load(\"./data/avro/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
